{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W205 - Fall 2019 - Project 3 - Understanding User Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nobu Yamaguchi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "- I am a data scientist at a game development company. \n",
    "- Our latest mobile game has two events we are interested in tracking: `purchase a sword` & `join guild`\n",
    "- Each has metadata characteristic of such events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"events\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore out events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = raw_events.select(raw_events.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "extracted_events = events.rdd.map(lambda x: json.loads(x.value)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+--------------+\n",
      "|Accept|          Host| User-Agent|    event_type|\n",
      "+------+--------------+-----------+--------------+\n",
      "|   */*|localhost:5000|curl/7.47.0|       default|\n",
      "|   */*|localhost:5000|curl/7.47.0|purchase_sword|\n",
      "|   */*|localhost:5000|curl/7.47.0|purchase_knife|\n",
      "|   */*|localhost:5000|curl/7.47.0| purchase_frog|\n",
      "|   */*|localhost:5000|curl/7.47.0|    ride_horse|\n",
      "|   */*|localhost:5000|curl/7.47.0|climb_mountain|\n",
      "|   */*|localhost:5000|curl/7.47.0|    ride_horse|\n",
      "|   */*|localhost:5000|curl/7.47.0| purchase_frog|\n",
      "+------+--------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_events \\\n",
    "        .write \\\n",
    "        .parquet(\"/tmp/extracted_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                 raw|           timestamp|              munged|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{\"Host\": \"localho...|2019-11-10 05:04:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:05:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:06:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:07:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:07:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:07:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:41:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:42:...|{\"Host\": \"moe\", \"...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|Accept|Cache-Control|Host| User-Agent|    event_type|           timestamp|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|   */*|     no-cache| moe|curl/7.47.0|       default|2019-11-10 05:04:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|purchase_sword|2019-11-10 05:05:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|purchase_knife|2019-11-10 05:06:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0| purchase_frog|2019-11-10 05:07:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|    ride_horse|2019-11-10 05:07:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|climb_mountain|2019-11-10 05:07:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|    ride_horse|2019-11-10 05:41:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0| purchase_frog|2019-11-10 05:42:...|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf('string')\n",
    "def munge_event(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    event['Host'] = \"moe\"\n",
    "    event['Cache-Control'] = \"no-cache\"\n",
    "    return json.dumps(event)\n",
    "\n",
    "raw_events = spark \\\n",
    "        .read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"endingOffsets\", \"latest\") \\\n",
    "        .load()\n",
    "\n",
    "munged_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .withColumn('munged', munge_event('raw'))\n",
    "munged_events.show()\n",
    "\n",
    "extracted_events = munged_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.munged))) \\\n",
    "    .toDF()\n",
    "extracted_events.show()\n",
    "\n",
    "extracted_events \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/extracted_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|Accept|Cache-Control|Host| User-Agent|    event_type|           timestamp|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|   */*|     no-cache| moe|curl/7.47.0|purchase_sword|2019-11-10 05:05:...|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "\n",
      "+------+-------------+----+-----------+----------+--------------------+\n",
      "|Accept|Cache-Control|Host| User-Agent|event_type|           timestamp|\n",
      "+------+-------------+----+-----------+----------+--------------------+\n",
      "|   */*|     no-cache| moe|curl/7.47.0|   default|2019-11-10 05:04:...|\n",
      "+------+-------------+----+-----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# separate.py\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf('string')\n",
    "def munge_event(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    event['Host'] = \"moe\"\n",
    "    event['Cache-Control'] = \"no-cache\"\n",
    "    return json.dumps(event)\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "munged_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .withColumn('munged', munge_event('raw'))\n",
    "\n",
    "extracted_events = munged_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.munged))) \\\n",
    "    .toDF()\n",
    "\n",
    "sword_purchases = extracted_events \\\n",
    "    .filter(extracted_events.event_type == 'purchase_sword')\n",
    "sword_purchases.show()\n",
    "sword_purchases \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/sword_purchases\")\n",
    "    \n",
    "default_hits = extracted_events \\\n",
    "    .filter(extracted_events.event_type == 'default')\n",
    "default_hits.show()\n",
    "default_hits \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/default_hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered event (purchase sword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_purchase('raw'))\n",
    "\n",
    "extracted_purchase_events = purchase_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_purchase_events.printSchema()\n",
    "extracted_purchase_events.show()\n",
    "\n",
    "extracted_purchase_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/purchases')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries from Spark\n",
    "(First, I read parquet from '/tmp/purchases'. Next, I ran the sql to select rows where Host is 'user1.comcast.com'. Finally, I showed the summary of dataframe.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accept</th>\n",
       "      <th>Host</th>\n",
       "      <th>User-Agent</th>\n",
       "      <th>event_type</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>*/*</td>\n",
       "      <td>user1.comcast.com</td>\n",
       "      <td>ApacheBench/2.3</td>\n",
       "      <td>purchase_sword</td>\n",
       "      <td>2019-11-24 18:08:27.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accept               Host       User-Agent      event_type  \\\n",
       "count      10                 10               10              10   \n",
       "unique      1                  1                1               1   \n",
       "top       */*  user1.comcast.com  ApacheBench/2.3  purchase_sword   \n",
       "freq       10                 10               10              10   \n",
       "\n",
       "                      timestamp  \n",
       "count                        10  \n",
       "unique                       10  \n",
       "top     2019-11-24 18:08:27.796  \n",
       "freq                          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases = spark.read.parquet('/tmp/purchases')\n",
    "\n",
    "purchases.show()\n",
    "\n",
    "purchases.registerTempTable('purchases')\n",
    "\n",
    "purchases_by_example2 = spark.sql(\"select * from purchases where Host = 'user1.comcast.com'\")\n",
    "\n",
    "purchases_by_example2.show()\n",
    "\n",
    "df = purchases_by_example2.toPandas()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-run the same one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_purchase('raw'))\n",
    "\n",
    "extracted_purchase_events = purchase_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_purchase_events.printSchema()\n",
    "extracted_purchase_events.show()\n",
    "\n",
    "extracted_purchase_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/purchases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Schema on definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet('/tmp/purchases')\n",
    "\n",
    "df.registerTempTable('purchases')\n",
    "\n",
    "query = \"create external table purchase_events stored as parquet location '/tmp/purchase_events' as select * from purchases\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "```\n",
    "Found 5 items\n",
    "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - hive   supergroup          0 2019-12-07 22:55 /tmp/hive\n",
    "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-07 23:29 /tmp/purchase_events\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-07 23:29 /tmp/purchases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/purchases/\n",
    "```\n",
    "```\n",
    "Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2019-12-07 23:29 /tmp/purchases/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup       1647 2019-12-07 23:29 /tmp/purchases/part-00000-32965ab8-c450-401a-bffe-8977a5723f4a-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Would like to see the table of purchase_sword events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from purchases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from purchase_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### A. See the tables above (from 2 tables). The answer is same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add another event filter (join_guild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|Accept|             Host|     User-Agent|event_type|           timestamp|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_join(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "join_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_join('raw'))\n",
    "\n",
    "extracted_join_events = join_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_join_events.printSchema()\n",
    "extracted_join_events.show()\n",
    "\n",
    "extracted_join_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/joins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.read.parquet('/tmp/joins')\n",
    "\n",
    "df2.registerTempTable('joins')\n",
    "\n",
    "query = \"create external table join_events stored as parquet location '/tmp/join_events' as select * from joins\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "```\n",
    "Found 7 items\n",
    "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - hive   supergroup          0 2019-12-07 22:55 /tmp/hive\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-07 23:37 /tmp/join_events\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-07 23:34 /tmp/joins\n",
    "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-07 23:29 /tmp/purchase_events\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-07 23:29 /tmp/purchases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/joins/\n",
    "```\n",
    "```\n",
    "Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2019-12-07 23:34 /tmp/joins/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup       1626 2019-12-07 23:34 /tmp/joins/part-00000-00480990-767d-4aa1-ab33-4f4e46fcbbac-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Would like to see the name of the hosts in the join_a_guild events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             host|\n",
      "+-----------------+\n",
      "|    user2.att.com|\n",
      "|user1.comcast.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(host) from joins\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             host|\n",
      "+-----------------+\n",
      "|    user2.att.com|\n",
      "|user1.comcast.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(host) from join_events\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### A. See the tables above (from 2 tables). The answer is same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create external table purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:09:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-07 23:10:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_purchase('raw'))\n",
    "\n",
    "extracted_purchase_events = purchase_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_purchase_events.printSchema()\n",
    "extracted_purchase_events.show()\n",
    "\n",
    "extracted_purchase_events.registerTempTable(\"extracted_purchase_events\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    create external table purchases\n",
    "    stored as parquet\n",
    "    location '/tmp/purchases'\n",
    "    as\n",
    "    select * from extracted_purchase_events\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create external table joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|Accept|             Host|     User-Agent|event_type|           timestamp|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|join_guild|2019-12-07 23:10:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|join_guild|2019-12-07 23:11:...|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_join(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "join_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_join('raw'))\n",
    "\n",
    "extracted_join_events = join_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_join_events.printSchema()\n",
    "extracted_join_events.show()\n",
    "\n",
    "extracted_join_events.registerTempTable(\"extracted_join_guilds\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    create external table joins\n",
    "    stored as parquet\n",
    "    location '/tmp/joins'\n",
    "    as\n",
    "    select * from extracted_join_guilds\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the program below during streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d36115b79a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msword_purchases\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/checkpoints_for_sword_purchases\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/sword_purchases\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"10 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0msink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_sword_purchase(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()\n",
    "\n",
    "sword_purchases = raw_events \\\n",
    "    .filter(is_sword_purchase(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_sword_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "sink = sword_purchases \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints_for_sword_purchases\") \\\n",
    "    .option(\"path\", \"/tmp/sword_purchases\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "sink.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/sword_purchases\n",
    "```\n",
    "```\n",
    "Found 10 items\n",
    "drwxr-xr-x   - root supergroup          0 2019-12-07 23:59 /tmp/sword_purchases/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup       2377 2019-12-07 23:59 /tmp/sword_purchases/part-00000-1cd80acb-be29-421f-9504-366ad83ee0fe-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2566 2019-12-07 23:59 /tmp/sword_purchases/part-00000-25b89790-6715-4e04-9085-40c9eba6b838-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2639 2019-12-07 23:57 /tmp/sword_purchases/part-00000-3c2d2b47-55a4-435f-a3a6-5ee971cc0322-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2381 2019-12-07 23:58 /tmp/sword_purchases/part-00000-3dd16d60-045b-4e00-a844-2e50b0df0714-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2513 2019-12-07 23:59 /tmp/sword_purchases/part-00000-418275e2-942a-4aa0-ab1c-4684c32e3429-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup        688 2019-12-07 23:57 /tmp/sword_purchases/part-00000-4cdf50b0-f6fe-4fa5-866d-a0d778cc96c9-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2378 2019-12-07 23:57 /tmp/sword_purchases/part-00000-4e5a34d5-c0f0-4227-b0ea-7a4849fc8f3e-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2571 2019-12-07 23:58 /tmp/sword_purchases/part-00000-6a559b52-0a73-4e61-9be0-5878fd299e05-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2440 2019-12-07 23:58 /tmp/sword_purchases/part-00000-d83457f0-b2cc-4410-a3eb-53f889f91416-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet('/tmp/sword_purchases')\n",
    "\n",
    "df.registerTempTable('sword_purchases')\n",
    "\n",
    "query = \"create external table sword_purchase_events stored as parquet location '/tmp/sword_purchase_events' as select * from sword_purchases\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Would like to see how many events are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1530|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from sword_purchase_events').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1530|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from sword_purchases').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### 1530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the program below during streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-50fd1a2ea39f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguild_joins\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/checkpoints_for_guild_joins\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/guild_joins\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"10 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0msink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def join_guild_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_guild_join(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()\n",
    "\n",
    "guild_joins = raw_events \\\n",
    "    .filter(is_guild_join(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      join_guild_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "sink = guild_joins \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints_for_guild_joins\") \\\n",
    "    .option(\"path\", \"/tmp/guild_joins\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "sink.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.read.parquet('/tmp/guild_joins')\n",
    "\n",
    "df2.registerTempTable('guild_joins')\n",
    "\n",
    "query = \"create external table guild_join_events stored as parquet location '/tmp/guild_join_events' as select * from guild_joins\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Would like to see how many guild join events are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     210|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from guild_join_events').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     210|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) from guild_joins').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### 210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Would like to know where the host was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             host|\n",
      "+-----------------+\n",
      "|user1.comcast.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct(host) from guild_join_events').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             host|\n",
      "+-----------------+\n",
      "|user1.comcast.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct(host) from guild_joins').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### user1.comcast.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-daef2895025b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msword_purchases\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_sword_purchase(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()\n",
    "\n",
    "sword_purchases = raw_events \\\n",
    "    .filter(is_sword_purchase(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_sword_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "query = sword_purchases \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
