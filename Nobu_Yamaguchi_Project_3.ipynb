{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W205 - Fall 2019 - Project 3 - Understanding User Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nobu Yamaguchi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "- I am a data scientist at a game development company. \n",
    "- Our latest mobile game has two events we are interested in tracking: `buy a sword` & `join guild`\n",
    "- Each has metadata characteristic of such events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"events\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore out events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = raw_events.select(raw_events.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "extracted_events = events.rdd.map(lambda x: json.loads(x.value)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+--------------+\n",
      "|Accept|          Host| User-Agent|    event_type|\n",
      "+------+--------------+-----------+--------------+\n",
      "|   */*|localhost:5000|curl/7.47.0|       default|\n",
      "|   */*|localhost:5000|curl/7.47.0|purchase_sword|\n",
      "|   */*|localhost:5000|curl/7.47.0|purchase_knife|\n",
      "|   */*|localhost:5000|curl/7.47.0| purchase_frog|\n",
      "|   */*|localhost:5000|curl/7.47.0|    ride_horse|\n",
      "|   */*|localhost:5000|curl/7.47.0|climb_mountain|\n",
      "|   */*|localhost:5000|curl/7.47.0|    ride_horse|\n",
      "|   */*|localhost:5000|curl/7.47.0| purchase_frog|\n",
      "+------+--------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_events \\\n",
    "        .write \\\n",
    "        .parquet(\"/tmp/extracted_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                 raw|           timestamp|              munged|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{\"Host\": \"localho...|2019-11-10 05:04:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:05:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:06:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:07:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:07:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:07:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:41:...|{\"Host\": \"moe\", \"...|\n",
      "|{\"Host\": \"localho...|2019-11-10 05:42:...|{\"Host\": \"moe\", \"...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|Accept|Cache-Control|Host| User-Agent|    event_type|           timestamp|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|   */*|     no-cache| moe|curl/7.47.0|       default|2019-11-10 05:04:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|purchase_sword|2019-11-10 05:05:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|purchase_knife|2019-11-10 05:06:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0| purchase_frog|2019-11-10 05:07:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|    ride_horse|2019-11-10 05:07:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|climb_mountain|2019-11-10 05:07:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0|    ride_horse|2019-11-10 05:41:...|\n",
      "|   */*|     no-cache| moe|curl/7.47.0| purchase_frog|2019-11-10 05:42:...|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf('string')\n",
    "def munge_event(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    event['Host'] = \"moe\"\n",
    "    event['Cache-Control'] = \"no-cache\"\n",
    "    return json.dumps(event)\n",
    "\n",
    "raw_events = spark \\\n",
    "        .read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"endingOffsets\", \"latest\") \\\n",
    "        .load()\n",
    "\n",
    "munged_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .withColumn('munged', munge_event('raw'))\n",
    "munged_events.show()\n",
    "\n",
    "extracted_events = munged_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.munged))) \\\n",
    "    .toDF()\n",
    "extracted_events.show()\n",
    "\n",
    "extracted_events \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/extracted_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|Accept|Cache-Control|Host| User-Agent|    event_type|           timestamp|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "|   */*|     no-cache| moe|curl/7.47.0|purchase_sword|2019-11-10 05:05:...|\n",
      "+------+-------------+----+-----------+--------------+--------------------+\n",
      "\n",
      "+------+-------------+----+-----------+----------+--------------------+\n",
      "|Accept|Cache-Control|Host| User-Agent|event_type|           timestamp|\n",
      "+------+-------------+----+-----------+----------+--------------------+\n",
      "|   */*|     no-cache| moe|curl/7.47.0|   default|2019-11-10 05:04:...|\n",
      "+------+-------------+----+-----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# separate.py\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf('string')\n",
    "def munge_event(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    event['Host'] = \"moe\"\n",
    "    event['Cache-Control'] = \"no-cache\"\n",
    "    return json.dumps(event)\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "munged_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .withColumn('munged', munge_event('raw'))\n",
    "\n",
    "extracted_events = munged_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.munged))) \\\n",
    "    .toDF()\n",
    "\n",
    "sword_purchases = extracted_events \\\n",
    "    .filter(extracted_events.event_type == 'purchase_sword')\n",
    "sword_purchases.show()\n",
    "sword_purchases \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/sword_purchases\")\n",
    "    \n",
    "default_hits = extracted_events \\\n",
    "    .filter(extracted_events.event_type == 'default')\n",
    "default_hits.show()\n",
    "default_hits \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/default_hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered event (purchase sword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_purchase('raw'))\n",
    "\n",
    "extracted_purchase_events = purchase_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_purchase_events.printSchema()\n",
    "extracted_purchase_events.show()\n",
    "\n",
    "extracted_purchase_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/purchases')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries from Spark\n",
    "(First, I read parquet from '/tmp/purchases'. Next, I ran the sql to select rows where Host is 'user1.comcast.com'. Finally, I showed the summary of dataframe.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:13:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-11-24 18:08:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accept</th>\n",
       "      <th>Host</th>\n",
       "      <th>User-Agent</th>\n",
       "      <th>event_type</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>*/*</td>\n",
       "      <td>user1.comcast.com</td>\n",
       "      <td>ApacheBench/2.3</td>\n",
       "      <td>purchase_sword</td>\n",
       "      <td>2019-11-24 18:08:27.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accept               Host       User-Agent      event_type  \\\n",
       "count      10                 10               10              10   \n",
       "unique      1                  1                1               1   \n",
       "top       */*  user1.comcast.com  ApacheBench/2.3  purchase_sword   \n",
       "freq       10                 10               10              10   \n",
       "\n",
       "                      timestamp  \n",
       "count                        10  \n",
       "unique                       10  \n",
       "top     2019-11-24 18:08:27.796  \n",
       "freq                          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases = spark.read.parquet('/tmp/purchases')\n",
    "\n",
    "purchases.show()\n",
    "\n",
    "purchases.registerTempTable('purchases')\n",
    "\n",
    "purchases_by_example2 = spark.sql(\"select * from purchases where Host = 'user1.comcast.com'\")\n",
    "\n",
    "purchases_by_example2.show()\n",
    "\n",
    "df = purchases_by_example2.toPandas()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_purchase('raw'))\n",
    "\n",
    "extracted_purchase_events = purchase_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_purchase_events.printSchema()\n",
    "extracted_purchase_events.show()\n",
    "\n",
    "extracted_purchase_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/purchases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Schema on definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet('/tmp/purchases')\n",
    "\n",
    "df.registerTempTable('purchases')\n",
    "\n",
    "query = \"create external table purchase_events stored as parquet location '/tmp/purchase_events' as select * from purchases\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "```\n",
    "Found 5 items\n",
    "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - hive   supergroup          0 2019-12-06 04:06 /tmp/hive\n",
    "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-06 04:21 /tmp/purchase_events\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-06 04:18 /tmp/purchases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/purchases/\n",
    "```\n",
    "```\n",
    "Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2019-12-06 04:45 /tmp/purchases/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup       1657 2019-12-06 04:45 /tmp/purchases/part-00000-0b79c2c0-8a64-4296-a6d7-62774ce4b4df-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add another event filter (ride_horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|Accept|             Host|     User-Agent|event_type|           timestamp|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_ride(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'ride_horse':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_ride('raw'))\n",
    "\n",
    "extracted_purchase_events = purchase_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_purchase_events.printSchema()\n",
    "extracted_purchase_events.show()\n",
    "\n",
    "extracted_purchase_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/rides')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = spark.read.parquet('/tmp/rides')\n",
    "\n",
    "df2.registerTempTable('rides')\n",
    "\n",
    "query = \"create external table ride_events stored as parquet location '/tmp/ride_events' as select * from rides\"\n",
    "\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "```\n",
    "Found 7 items\n",
    "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - hive   supergroup          0 2019-12-06 04:39 /tmp/hive\n",
    "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-06 04:45 /tmp/purchase_events\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-06 04:45 /tmp/purchases\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-06 04:45 /tmp/ride_events\n",
    "drwxr-xr-x   - root   supergroup          0 2019-12-06 04:45 /tmp/rides\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/rides/\n",
    "```\n",
    "```\n",
    "Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2019-12-06 04:45 /tmp/rides/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup       1623 2019-12-06 04:45 /tmp/rides/part-00000-669d6a5a-2ca8-4c3a-b90a-0122025dc974-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:43:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2019-12-06 04:44:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "purchase_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_purchase('raw'))\n",
    "\n",
    "extracted_purchase_events = purchase_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_purchase_events.printSchema()\n",
    "extracted_purchase_events.show()\n",
    "\n",
    "extracted_purchase_events.registerTempTable(\"extracted_purchase_events\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    create external table purchases\n",
    "    stored as parquet\n",
    "    location '/tmp/purchases'\n",
    "    as\n",
    "    select * from extracted_purchase_events\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|Accept|             Host|     User-Agent|event_type|           timestamp|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|ride_horse|2019-12-06 04:43:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|ride_horse|2019-12-06 04:44:...|\n",
      "+------+-----------------+---------------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_ride(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'ride_horse':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "ride_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('raw'),\n",
    "            raw_events.timestamp.cast('string')) \\\n",
    "    .filter(is_ride('raw'))\n",
    "\n",
    "extracted_ride_events = ride_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "    .toDF()\n",
    "extracted_ride_events.printSchema()\n",
    "extracted_ride_events.show()\n",
    "\n",
    "extracted_ride_events.registerTempTable(\"extracted_ride_horses\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    create external table rides\n",
    "    stored as parquet\n",
    "    location '/tmp/rides'\n",
    "    as\n",
    "    select * from extracted_ride_horses\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-daef2895025b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msword_purchases\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_sword_purchase(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()\n",
    "\n",
    "sword_purchases = raw_events \\\n",
    "    .filter(is_sword_purchase(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_sword_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "query = sword_purchases \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_sword_purchase(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load()\n",
    "\n",
    "sword_purchases = raw_events \\\n",
    "    .filter(is_sword_purchase(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_sword_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "sink = sword_purchases \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints_for_sword_purchases\") \\\n",
    "    .option(\"path\", \"/tmp/sword_purchases\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "sink.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ docker-compose exec cloudera hadoop fs -ls /tmp/sword_purchases\n",
    "```\n",
    "```\n",
    "Found 3 items\n",
    "drwxr-xr-x   - root supergroup          0 2019-12-06 05:27 /tmp/sword_purchases/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup        688 2019-12-06 05:27 /tmp/sword_purchases/part-00000-ad0bdd9f-fb91-4f41-a067-27c4b736e019-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2459 2019-12-06 05:27 /tmp/sword_purchases/part-00000-ccedace1-506b-4560-bb92-3bf6b7b23868-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
